# Google Search Console Indexing Issues - Comprehensive Analysis
**Date:** December 23, 2025  
**Website:** https://shalean.co.za  
**Status:** Analysis Only - No Changes Made

---

## üìä Executive Summary

Based on the Search Console data showing 303 pages not indexed and an alert stating "Due to internal issues, this report has not been updated to reflect recent data", this document identifies all potential issues preventing proper indexing and performance reporting.

**Key Findings:**
- 79 pages: "Crawled - currently not indexed" (Most Critical)
- 139 pages: "Not found (404)" 
- 30 pages: "Blocked due to access forbidden (403)"
- 19 pages: "Page with redirect"
- 17 pages: "Soft 404"
- 6 pages: "Discovered - currently not indexed"

---

## üî¥ CRITICAL ISSUES

### 1. **Crawled - Currently Not Indexed (79 pages)** ‚ö†Ô∏è HIGHEST PRIORITY

**Problem:** Google can crawl these pages but chooses not to index them. This is the largest single issue.

**Common Causes:**
- **Thin/duplicate content**: Location pages may have very similar or identical content
- **Low quality signals**: Pages may lack sufficient unique, valuable content
- **Crawl budget waste**: Too many similar pages competing for indexing priority
- **Content quality**: Pages might not meet Google's quality threshold

**Likely Affected Pages:**
- Location/suburb pages with minimal unique content
- Service pages with template-based content
- Blog posts with insufficient content depth

**Recommendations:**
1. **Add unique content** to each location page (300-500 words minimum)
2. **Differentiate location pages** with:
   - Local landmarks and neighborhoods
   - Location-specific testimonials
   - Unique service details per area
   - Local market insights
3. **Consolidate similar pages** if they can't be made unique
4. **Review content depth** - ensure each indexed page provides unique value

---

### 2. **404 Errors (139 pages)** ‚ö†Ô∏è HIGH PRIORITY

**Problem:** Large number of pages returning 404 Not Found errors.

**Root Causes Identified:**
1. **Legacy URLs still in sitemap**: Old URLs that no longer exist
2. **Broken internal links**: Links pointing to deleted/moved pages
3. **Redirect chains**: URLs that redirect multiple times before 404
4. **Old indexed URLs**: Pages removed but still in Google's index

**Files to Check:**
- `app/sitemap.ts` - Verify all URLs in sitemap are valid
- `next.config.js` - Ensure all redirects are properly configured
- Internal links across components and pages

**Recommendations:**
1. **Audit sitemap**: Ensure `sitemap.xml` only includes existing pages
2. **Fix broken redirects**: Check `next.config.js` redirects are working
3. **Add 301 redirects** for all removed pages to nearest equivalent
4. **Remove old URLs** from sitemap if no redirect target exists
5. **Use 410 Gone** for permanently removed content (better than 404)

**Note:** Previous remediation work was done (see `docs/404-remediation-summary.md`) but 139 pages still return 404, suggesting:
- Sitemap may still include invalid URLs
- External sites linking to old URLs
- Redirect rules not catching all variations

---

### 3. **403 Forbidden Errors (30 pages)** ‚ö†Ô∏è MEDIUM-HIGH PRIORITY

**Problem:** Pages returning 403 Access Forbidden.

**Potential Causes:**
1. **API routes accessible to crawlers**:
   - `/api/admin/*` routes requiring authentication
   - `/api/services/popular` may return 403 on database errors
   - Any API route that doesn't properly handle unauthenticated requests

2. **Server-side access control issues**:
   - Middleware blocking Googlebot
   - Authentication checks failing for crawlers
   - IP-based blocking

**Current Configuration:**
- `app/robots.ts` disallows `/api/` - ‚úÖ Good
- `public/robots.txt` also disallows `/api/` - ‚úÖ Good
- But Next.js uses `app/robots.ts` (dynamic), not `public/robots.txt`

**Issue Identified:**
- Both `public/robots.txt` and `app/robots.ts` exist - potential conflict
- Next.js prioritizes dynamic `robots.ts` over static `robots.txt`
- If `robots.ts` fails, crawlers may fall back to `robots.txt` which might be outdated

**Recommendations:**
1. **Remove `public/robots.txt`** to avoid confusion (Next.js uses `app/robots.ts`)
2. **Verify API routes** aren't being accessed directly:
   - Check if any `/api/*` URLs are in sitemap (shouldn't be)
   - Verify no internal links point to API routes
3. **Review error handling** in API routes:
   - `app/api/services/popular/route.ts` returns 403 on RLS errors
   - Ensure proper error responses don't confuse crawlers
4. **Check for middleware** that might block Googlebot (none found currently)

---

### 4. **Page with Redirect (19 pages)** ‚ö†Ô∏è MEDIUM PRIORITY

**Problem:** Pages that redirect to other pages. While redirects are fine, issues arise when:
- Redirect chains (A ‚Üí B ‚Üí C)
- Temporary redirects (302 instead of 301)
- Redirect loops
- Redirects to non-indexable pages

**Current Redirect Configuration:**
- `next.config.js` has extensive redirect rules (400+ lines)
- All configured as `permanent: true` (301 redirects) - ‚úÖ Good
- Redirects from old URLs to new structure - ‚úÖ Good

**Potential Issues:**
1. **Redirect chains**: Old URL ‚Üí intermediate ‚Üí final destination
2. **Redirects in sitemap**: If sitemap includes URLs that redirect
3. **Soft redirects**: JavaScript redirects instead of HTTP redirects

**Recommendations:**
1. **Audit redirect chains**: Ensure no A ‚Üí B ‚Üí C chains exist
2. **Remove redirecting URLs from sitemap**: Only include final destinations
3. **Use direct redirects**: Point old URLs directly to final destination
4. **Verify all redirects work**: Test redirect URLs manually

---

### 5. **Soft 404 (17 pages)** ‚ö†Ô∏è MEDIUM PRIORITY

**Problem:** Pages that return HTTP 200 but have no meaningful content (like a 404 page).

**Common Causes:**
1. **Empty or error pages** returning 200 status
2. **Generic error messages** without proper 404 status
3. **Template pages** with no content
4. **Dynamic routes** with missing data that show empty pages

**Likely Affected:**
- Location pages with missing data
- Blog posts that fail to load but return 200
- Dynamic routes that should return 404 but don't

**Recommendations:**
1. **Use proper 404 responses**: Return `notFound()` or 404 status for missing content
2. **Review location pages**: Ensure they handle missing data correctly
3. **Check blog post pages**: Verify they return 404 for non-existent posts
4. **Audit dynamic routes**: Ensure error states return proper status codes

---

### 6. **Alternative Page with Proper Canonical Tag (7 pages)** ‚ö†Ô∏è LOW PRIORITY

**Problem:** Pages with canonical tags pointing to other pages. These won't be indexed (by design).

**Status:** This is likely **intentional** - canonical tags are used to avoid duplicate content.

**Recommendations:**
1. **Verify canonical tags are correct**: Ensure they point to the preferred version
2. **Check for incorrect canonicals**: Pages that shouldn't have canonicals but do

---

### 7. **Duplicate without User-Selected Canonical (4 pages)** ‚ö†Ô∏è LOW PRIORITY

**Problem:** Duplicate content where Google can't determine the canonical version.

**Recommendations:**
1. **Add canonical tags** to duplicate pages
2. **Consolidate duplicate content** if possible
3. **Use hreflang** if duplicates are for different languages/regions

---

### 8. **Blocked by robots.txt (2 pages)** ‚ö†Ô∏è LOW PRIORITY

**Problem:** Pages explicitly blocked in robots.txt.

**Current Configuration:**
- `app/robots.ts` disallows:
  - `/api/`
  - `/admin/`
  - `/dashboard/`
  - `/cleaner/`
  - `/booking/confirmation`
  - `/_next/`
  - `/static/`

**Status:** These are likely intentional blocks for private/admin pages.

**Recommendations:**
1. **Verify blocked pages should be blocked**: Confirm these shouldn't be indexed
2. **Check if any public pages are accidentally blocked**: Review robots.ts rules

---

### 9. **Discovered - Currently Not Indexed (6 pages)** ‚ö†Ô∏è LOW PRIORITY

**Problem:** Google discovered these pages but hasn't crawled/indexed them yet.

**Common Causes:**
- New pages not yet crawled
- Low priority pages
- Pages with crawl budget constraints

**Recommendations:**
1. **Request indexing** in Search Console for important new pages
2. **Improve internal linking** to increase discovery priority
3. **Be patient** - this often resolves itself as Google recrawls

---

## üîß TECHNICAL CONFIGURATION ISSUES

### 1. **Robots.txt Duplication** ‚ö†Ô∏è MEDIUM PRIORITY

**Issue:** Both `public/robots.txt` (static) and `app/robots.ts` (dynamic) exist.

**Problem:**
- Next.js uses `app/robots.ts` when it exists
- If `robots.ts` fails or has issues, crawlers may fall back to `robots.txt`
- Two sources of truth can cause confusion
- The static `robots.txt` may become outdated

**Current State:**
- `public/robots.txt`: Simple static file, less detailed
- `app/robots.ts`: Dynamic, more comprehensive rules

**Recommendation:**
- **Remove `public/robots.txt`** since Next.js uses `app/robots.ts`
- Or ensure both are identical (not recommended - maintenance burden)

---

### 2. **Sitemap Configuration** ‚ö†Ô∏è MEDIUM PRIORITY

**Current State:**
- Sitemap generated dynamically via `app/sitemap.ts`
- References in `app/robots.ts`: `sitemap: 'https://shalean.co.za/sitemap.xml'`
- References in `public/robots.txt`: `Sitemap: https://shalean.co.za/sitemap.xml`

**Potential Issues:**
1. **Sitemap includes redirecting URLs**: Should only include final destinations
2. **Sitemap includes 404 pages**: Should only include existing pages
3. **Sitemap too large**: If >50,000 URLs, needs to be split
4. **Sitemap generation errors**: If `getPublishedPosts()` or location data fails

**Recommendations:**
1. **Verify sitemap accessibility**: Test `https://shalean.co.za/sitemap.xml` loads correctly
2. **Audit sitemap contents**: Ensure no 404 or redirect URLs included
3. **Check for generation errors**: Review logs for sitemap generation failures
4. **Validate sitemap format**: Use XML sitemap validators

---

### 3. **Cache Headers** ‚ö†Ô∏è LOW PRIORITY

**Current Configuration (`proxy.ts`):**
- Location and blog pages: `Cache-Control: public, s-maxage=3600, stale-while-revalidate=86400`
- This is generally good for SEO

**Potential Issue:**
- If cache is too aggressive, Google may not see fresh content
- `stale-while-revalidate=86400` (24 hours) might be too long

**Recommendation:**
- Current settings are reasonable
- Monitor if content updates aren't reflected in search results

---

### 4. **Security Headers** ‚úÖ GOOD

**Current Configuration:**
- `X-Content-Type-Options: nosniff`
- `X-Frame-Options: DENY`
- `X-XSS-Protection: 1; mode=block`
- `Referrer-Policy: strict-origin-when-cross-origin`

**Status:** These are good for security and don't interfere with crawling.

---

## üìã CONTENT QUALITY ISSUES

### 1. **Thin Content on Location Pages** ‚ö†Ô∏è HIGH PRIORITY

**Problem:** Many location pages likely have minimal unique content, causing "Crawled - not indexed" status.

**Evidence:**
- SEO audit report mentions "thin or duplicate content" on location pages
- 159 location page files exist
- Template-based pages may have similar content

**Recommendations:**
1. **Add unique content** (300-500 words) to each location page:
   - Local area descriptions
   - Neighborhood-specific information
   - Local landmarks and points of interest
   - Area-specific service details
2. **Differentiate pages**: Ensure each page offers unique value
3. **Add local testimonials**: Include area-specific customer reviews
4. **Create location-specific FAQs**: Address local concerns

---

### 2. **Meta Tags and Structured Data** ‚úÖ MOSTLY GOOD

**Current State:**
- Metadata properly configured via `lib/metadata.ts`
- Structured data (JSON-LD) implemented
- Open Graph and Twitter cards configured

**Potential Issues:**
- Some pages may lack unique meta descriptions
- Location pages might have duplicate descriptions

**Recommendations:**
1. **Ensure unique meta descriptions** for each page (120-160 characters)
2. **Verify structured data** is valid (use Google's Rich Results Test)
3. **Add location-specific structured data** where applicable

---

## üîç CRAWLABILITY ISSUES

### 1. **No Middleware Blocking** ‚úÖ GOOD

**Finding:** No `middleware.ts` file found that might block crawlers.

**Status:** No middleware blocking issues identified.

---

### 2. **Authentication Checks** ‚ö†Ô∏è MEDIUM PRIORITY

**Issue:** Some API routes check for admin authentication and return 403.

**Affected Routes:**
- All `/api/admin/*` routes
- Some routes return 403 on database permission errors

**Current Protection:**
- `robots.txt` disallows `/api/` - ‚úÖ Good
- But if any API URLs are linked or in sitemap, crawlers may try them

**Recommendations:**
1. **Verify no API URLs in sitemap** (shouldn't be)
2. **Check for internal links to API routes** (shouldn't exist)
3. **Ensure API routes return proper errors** (don't confuse crawlers)

---

### 3. **JavaScript Rendering** ‚ö†Ô∏è LOW PRIORITY

**Current State:**
- Next.js App Router with Server-Side Rendering (SSR)
- Most content is server-rendered - ‚úÖ Good for SEO

**Potential Issues:**
- Some client components may not render for crawlers
- JavaScript-heavy interactions may hide content

**Recommendations:**
1. **Verify critical content is SSR** (not client-only)
2. **Test with JavaScript disabled** to ensure content is accessible
3. **Use server components** for SEO-critical content

---

## üìä SUMMARY OF ACTIONS NEEDED

### Immediate Actions (Critical)

1. **Fix "Crawled - not indexed" (79 pages)**:
   - Add unique content to location pages
   - Differentiate similar pages
   - Consolidate if unique content isn't possible

2. **Fix 404 errors (139 pages)**:
   - Audit sitemap for invalid URLs
   - Add redirects for removed pages
   - Remove broken URLs from sitemap

3. **Fix 403 errors (30 pages)**:
   - Verify no API URLs in sitemap
   - Check for broken links to API routes
   - Review error handling in API routes

### Short-term Actions (High Priority)

4. **Fix redirect issues (19 pages)**:
   - Audit redirect chains
   - Remove redirecting URLs from sitemap
   - Verify all redirects work correctly

5. **Fix soft 404s (17 pages)**:
   - Ensure missing content returns proper 404
   - Review error handling in dynamic routes
   - Test location/blog pages with missing data

6. **Remove robots.txt duplication**:
   - Delete `public/robots.txt` (keep `app/robots.ts`)

### Long-term Actions (Medium Priority)

7. **Improve content quality**:
   - Add unique content to all location pages
   - Differentiate similar service pages
   - Enhance blog post content depth

8. **Monitor and maintain**:
   - Regular sitemap audits
   - Monitor Search Console for new issues
   - Keep redirect rules updated

---

## üîó REFERENCE FILES

- `app/robots.ts` - Dynamic robots.txt configuration
- `public/robots.txt` - Static robots.txt (should be removed)
- `app/sitemap.ts` - Sitemap generation
- `next.config.js` - Redirect configuration
- `proxy.ts` - Response headers and caching
- `SEO_AUDIT_REPORT.md` - Previous SEO analysis
- `docs/404-remediation-summary.md` - Previous 404 fixes
- `data/404-resolution-plan.md` - 404 remediation plan

---

## üìù NOTES

- **Search Console Alert**: "Due to internal issues, this report has not been updated to reflect recent data" suggests Google-side issues, but local fixes are still needed.

- **Last Update**: Search Console shows "Last update: 23/12/2025" - verify this is current.

- **Indexing Stats**: 
  - 176 pages indexed ‚úÖ
  - 303 pages not indexed ‚ö†Ô∏è
  - Focus on the 79 "Crawled - not indexed" as highest priority

- **No code changes made** per user request - this is analysis only.

